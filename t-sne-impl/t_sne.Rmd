---
title: "t_sne"
author: "Luis Angel Rodriguez Garcia"
date: "3/5/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

```{r, include=FALSE}
library(dplyr)
```

We are going to play with the Iris dataset:

```{r}
X <- iris %>% dplyr::select(-Species) %>% as.matrix()
n <- nrow(X)
p <- ncol(X)
```

## Euclidean distance

The way to calculate the Euclidean distance:

$$
\|\mathbf{x}_i-\mathbf{x}_j\|^2 = \|\mathbf{x}_i\|^2 + \|\mathbf{x}_j\|^2 - 2\|\mathbf{x}_i\|\|\mathbf{x}_j\|
$$

where $\|\mathbf{x}_i\| = \sqrt{x_{i,1}^2 + \dots + x_{i,p}^2}$ and  $\mathbf{x}_k^2 = \mathbf{x}_k'\mathbf{x}_k = x_{k1}^2 + ... + x_{kp}^2$.

The following method apply the formula above:

```{r iris}
x_diff <- function(X) {
  n <- nrow(X)
  sum_x <- apply(X^2, MARGIN=1, FUN=sum)
  sum_x_m <- t(matrix(replicate(n, sum_x), byrow=T, nrow=n))
  cross_times_minus_2 <- -2 * (X %*% t(X))
  D <- t(cross_times_minus_2 + sum_x_m) + sum_x_m
  D <- round(D, digits=4) 
}
```

## Perplexity

$$
Perp_i = 2^{H_i}
$$

Where $H_i$ is the Shannon entropy in the point $x_i$ of the conditional probability:

\begin{equation}
\begin{split}
H_i  & = - \sum_{j\neq i}p_{j|i}\log(p_{j|i}) \\
     & = - \sum_{j\neq i}p_{j|i}\log(p_{j,i}/p_{i}) \\
     & = - \sum_{j\neq i}p_{j|i}(\log(p_{j,i})-\log(p_{i})) \\
     & = - \sum_{j\neq i}p_{j|i}(\log(e^{-||x_i-x_j||^2/2\sigma^2})-\log(\sum_{k\neq i}e^{-||x_i-x_j||^2/2\sigma^2})) \\
     & = - \sum_{j\neq i}p_{j|i}((-||x_i-x_j||^2/2\sigma^2)-\log(\sum_{k\neq i}e^{-||x_i-x_j||^2/2\sigma^2})) \\
     & = \sum_{j\neq i}p_{j|i}(\log(S_i)+||x_i-x_j||^2\frac{1}{2\sigma^2}) \\
     & = \log(S_i)\sum_{j\neq i}p_{j|i} + \frac{1}{2\sigma^2}\sum_{j\neq i}p_{j|i} ||x_i-x_j||^2) \\
     & = \log(S_i)+\frac{1}{2\sigma^2}\sum_{j\neq i}p_{j|i}||x_i-x_j||^2
\end{split}
\end{equation}

Where $S_i=\sum_{k\neq i}e^{-||x_i-x_j||^2/2\sigma^2}$ and $\sum_{j\neq i}p_{j|i}=1$

In order to proceed with the optimization of the variance, we are going to define this term $\frac{1}{2\sigma^2}$ as the parameter $\beta$.

```{r}
entropy_beta <- function(D_i, beta=1) {
  P_i <- exp(-D_i * beta)
  sum_p_i <- sum(P_i)
  H_i <- log(sum_p_i) + (beta * sum(D_i * P_i) /sum_p_i) 
  P_i <- P_i / sum_p_i
  return(list(entropy=H_i, probs=P_i))
}
```

The goal is to adjust the variability so that the perplexity at each point is the same. The perplexity is a way to measure the effective number of neighbors of a point. We are going to perform a binary search to get the probabilities in such a way that the conditional Gaussian has the same perplexity.

```{r}
index_except_i <- function(i, n) {
  index <- c(seq(1,i-1),seq(i+1,n))
  if (i == 1) {
    index <- 2:n
  } else if (i == n) {
    index <- 1:(n-1)
  }
  return(index)
}

binary_search <- function(h_diff, beta, i, beta_min, beta_max) {
  if(h_diff > 0) {
    beta_min = beta[i]
    if(beta_max == -Inf || beta_max == Inf) {
      beta[i] <- beta[i] * 2
    } else {
      beta[i] <- (beta[i] + beta_max) / 2
    }
  } else {
    beta_max = beta[i]
    if(beta_min == -Inf || beta_min == Inf) {
      beta[i] <- beta[i] / 2
    } else {
      beta[i] <- (beta[i] + beta_min) / 2
    }
  }
  return(list(beta=beta, min=beta_min, max=beta_max))
}

binary_search_optimization <- function(D_i, i, beta, h_star, prob_star, log_perp, 
                                       tolerance=1e-5) {
  beta_min <- -Inf
  beta_max <- Inf
  tries <- 0
  h_diff <- h_star - log_perp
  
  while(abs(h_diff) > tolerance && tries < 50) {
    beta_opt <- binary_search(h_diff, beta, i, beta_min, beta_max)
    beta <- beta_opt$beta; beta_min <- beta_opt$min; beta_max <- beta_opt$max
    
    res_loop <- entropy_beta(D_i, beta[i])
    h_star <- res_loop$entropy; prob_star <- res_loop$probs
    
    h_diff <- h_star - log_perp
    tries <- tries + 1 
  }
  return(list(probs=prob_star, beta=beta))
}

```

Once we have defined these two methods, we are able to obtain the high dimensional properties:

```{r}

high_dimension_probs <- function(X=matrix(), tolerance=1e-5, perplexity=30) {
  n <- nrow(X)
  p <- ncol(X)
  
  D <- x_diff(X)
  
  P <- matrix(0, nrow=150, ncol=150)
  beta <- rep(1, n)
  log_perp <- log(perplexity)
  
  for(i in seq_len(n)) {
    column_index <- index_except_i(i, n)
    D_i <- D[i, column_index]
    
    res <- entropy_beta(D_i, beta[i])
    h_star <- res$entropy
    prob_star <- res$probs
    
    h_diff <- h_star - log_perp
    
    res_opt = binary_search_optimization(D_i, i, beta, h_star, prob_star, 
                                           log_perp, tolerance)
    prob_star = res_opt$probs; beta = res_opt$beta
    
    P[i, column_index] <- prob_star
  }
  print("Values of sigma for each x_i")
  print(sqrt(1/beta))
  print(sprintf("Mean value of sigma: %01.2f", mean(sqrt(1/beta))))
  return(P)
}
```

The version of the t-SNE is the symmetric one, that has the property that $p_{ij} = p_{ji}$ and $q_{ij}=q_{ji} \quad \forall i,j$. Therefore, we define $p_{ij}=\frac{p_{i|j}+p_{j|i}}{2n}$.

```{r}
symmetric_probs <- function(P) {
  P = (P + t(P)) / (2*nrow(P))
  return(P)
}
```

In order to initialize the lower dimension probability matrix, we are going to use the method `mvtnorm::rmvnorm` as it is described in the paper: $\mathbf{\mathcal{Y}}^{0} = \{y_1,...,y_n\} \sim \mathcal{N}(0, 10^{-4}\mathbf{I}_n)$ which is assigned to $\mathbf{\mathcal{Y}}^{1}$ and $\mathbf{\mathcal{Y}}^{2}$ (the first two initial states).


### Gradient Descent

$$
C = KL(P||Q) = \sum_{i}\sum_{j}p_{ij}\log{\frac{p_{ij}}{q_{ij}}}=\sum_i\sum_j p_{ij}\left(\log p_{ij}-\log{q_{ij}}\right)=\sum_i\sum_j p_{ij}\log{p_{ij}}-p_{ij}\log{q_{ij}}
$$

We define these two auxiliary variables $d_{ij}=\|y_i-y_j\|$ and $Z=\sum_{k\neq j}\left(1+d^2_{kl})^{-1}\right)$ 

$$
\frac{\partial C}{\partial y_i} = \sum_{j \neq i}\left[\frac{\partial C}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial y_i} +  \frac{\partial C}{\partial d_{ji}}\frac{\partial d_{ji}}{\partial y_i}\right]
$$

$$
\frac{\partial}{\partial \mathbf{y}} \|\mathbf{y}\|^2 = \frac{\partial}{\partial \mathbf{y}} \sum_{j=1}^p y_j^2 = \left(\frac{\partial}{\partial y_1} \sum_{j=1}^p y_j^2,\ldots,\frac{\partial}{\partial y_p} \sum_{j=1}^p y_j^2\right) = \left(2 y_1,\ldots, 2y_p\right) = 2 \mathbf{y}
$$

$$
\frac{\partial d_{ij}}{\partial y_i} = \frac{\partial}{\partial y_i} \|y_i-y_j\| = 
\frac{\partial}{\partial y_i} (\|y_i\|^2+\|y_j\|^2-2y_i'y_j)^{\frac{1}{2}} \underset{\frac{\partial}{\partial x}\sqrt{g(x)}=\frac{1}{2\sqrt{g(x)}}g'(x)}{=} 
\frac{1}{2}\frac{1}{d_{ij}} \frac{\partial}{\partial y_i} (\|y_i\|^2+\|y_j\|^2-2y_i'y_j) = 
\frac{1}{2}\frac{1}{d_{ij}} (2y_i-2y_j) = \frac{(y_i-y_j)}{d_{ij}}
$$

$$
\frac{\partial d_{ji}}{\partial y_i} = \frac{\partial}{\partial y_i} \|y_j-y_i\| = 
\frac{\partial}{\partial y_i} (\|y_j\|^2+\|y_i\|^2-2y_j'y_i)^{\frac{1}{2}} = 
\frac{1}{2}\frac{1}{d_{ji}} \frac{\partial}{\partial y_i} (\|y_j\|^2+\|y_i\|^2-2y_j'y_i) =
\frac{1}{2}\frac{1}{d_{ji}} (2y_i-2y_j) = 
\frac{y_j - y_i}{d_{ji}} \underset{d_{ij}=d_{ji}}{=} \frac{y_j - y_j}{d_{ij}} = 
\frac{\partial d_{ij}}{\partial y_i}
$$

```{r}
library(numDeriv)

dij.1 <- function(i, j) {
  sqrt(norm(i, type="2")^2 + norm(j, type="2")^2 - 2 * (t(i) %*% j))
}

dij.2 <- function(i, j) {
  norm(i-j, type="2")
}

y <- data.matrix(iris)[, -5]

# For rows 2 and 3
result1 <- as.numeric(round(jacobian(func=dij.2, x=y[2,], j=y[3,]), digits=7))
result2 <- as.numeric((y[2,]-y[3,])/norm(y[2,]-y[3,], type="2"))
all.equal(result2, result1) # checked
# For row 3 and 2
result3 <- as.numeric(round(jacobian(func=dij.2, x=y[3,], i=y[2,]), digits=7))
result4 <- as.numeric((y[3,]-y[2,])/norm(y[2,]-y[3,], type="2"))
all.equal(result4, result3) # checked
# Checked that both d(d_ij)/dy_i = d(d_ji)/dy_i
```


$$
\frac{\partial C}{\partial y_i} = 
\sum_{j \neq i}\left[\frac{\partial C}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial y_i} +  \frac{\partial C}{\partial d_{ji}}\frac{\partial d_{ji}}{\partial y_i}\right] \underset{d_{ij}=d_{ji}}{=} 
\sum_{j \neq i}\left[\frac{\partial d_{ij}}{\partial y_i}\left(\frac{\partial C}{\partial d_{ij}} +  \frac{\partial C}{\partial d_{ij}}\right)\right] =
2\sum_{j \neq i}\left[\frac{\partial C}{\partial d_{ij}}\right] \frac{y_i-y_j}{d_{ij}}
$$

```{r}

C <- function(P, Q) {
  n <- nrow(P)
  for(i in n)
    sum(P[i,]*log(P[i,]) - P[i,j]*log(Q[i,]))
}

data_p <- high_dimension_probs(y)
data_d <- data.matrix(dist(y))

data_q <- data_p * 3

n

```


